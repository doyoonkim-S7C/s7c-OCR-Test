import boto3
from botocore.config import Config
import base64
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from pdf2image import convert_from_path
import io
import os
from PyPDF2 import PdfReader, PdfWriter
from datetime import datetime
import argparse
import glob
from pathlib import Path
import time

def process_pdf_batch(pdf_path, start_page, end_page, batch_id):
    """ê° ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ - ì´ë¯¸ì§€ ë³€í™˜ ë°©ì‹"""
    print(f"ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹œì‘: í˜ì´ì§€ {start_page+1}-{end_page}")

    # ì‹œê°„ ì¸¡ì • ì‹œì‘
    batch_start_time = time.time()
    image_conversion_start = time.time()

    try:
        config = Config(read_timeout=1000)

        # AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        bedrock = boto3.client(
            service_name='bedrock-runtime',
            region_name='us-east-1',
            config=config,
        )

        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í•´ë‹¹ í˜ì´ì§€ ë²”ìœ„ë§Œ)
        print(f"ğŸ–¼ï¸  PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘... (í˜ì´ì§€ {start_page+1}-{end_page})")

        # pdf2imageë¡œ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_path(
            pdf_path,
            dpi=200,  # ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ DPI ì„¤ì •
            first_page=start_page + 1,  # pdf2imageëŠ” 1-based indexing
            last_page=end_page,
            fmt='PNG'
        )

        # ì´ë¯¸ì§€ë“¤ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ Claudeì—ê²Œ ì „ì†¡
        image_contents = []

        for i, image in enumerate(images):
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            img_byte_array = io.BytesIO()
            image.save(img_byte_array, format='PNG')
            img_byte_array = img_byte_array.getvalue()

            # base64 ì¸ì½”ë”©
            image_b64 = base64.standard_b64encode(img_byte_array).decode('utf-8')

            # ì´ë¯¸ì§€ ì»¨í…ì¸  ì¶”ê°€
            image_contents.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/png",
                    "data": image_b64
                }
            })

        # ì´ë¯¸ì§€ ë³€í™˜ ì‹œê°„ ì¸¡ì •
        image_conversion_time = time.time() - image_conversion_start
        print(f"âœ… {len(images)}ê°œ í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì™„ë£Œ ({image_conversion_time:.2f}ì´ˆ)")

        # ì¶”ë¡  ì‹œì‘ ì‹œê°„ ì¸¡ì •
        inference_start_time = time.time()

        # ë©”ì‹œì§€ ì»¨í…ì¸  êµ¬ì„± (ì´ë¯¸ì§€ë“¤ + í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸)
        content = []

        # ëª¨ë“  ì´ë¯¸ì§€ ì¶”ê°€
        content.extend(image_contents)

        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        content.append({
            "type": "text",
            "text": f"""Please convert all the content from these {len(images)} page images to markdown format.

IMPORTANT INSTRUCTIONS:
1. Process ALL content completely from all images - do not stop mid-way or use "##ê³„ì†" or similar continuation markers
2. Extract ALL text, including headers, body text, captions, and footnotes from each image
3. Convert tables to HTML format with proper structure
4. Maintain the original formatting and structure as much as possible
5. Process the images in order and include ALL information from every image
6. Do not truncate or summarize - provide the complete content
7. If multiple images are provided, process them as consecutive pages

Please ensure you process the entire content from all images without any continuation markers or incomplete outputs."""
        })

        # AWS Bedrock Claude ëª¨ë¸ í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 32768,  # ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }

        # AWS Bedrockì„ í†µí•œ Claude ëª¨ë¸ í˜¸ì¶œ
        response = bedrock.invoke_model(
            modelId='global.anthropic.claude-sonnet-4-5-20250929-v1:0',
            body=json.dumps(body),
            contentType='application/json'
        )

        # ì‘ë‹µ ì²˜ë¦¬
        response_body = json.loads(response['body'].read())
        result = response_body['content'][0]['text']

        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        inference_time = time.time() - inference_start_time
        batch_total_time = time.time() - batch_start_time

        print(f"ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì™„ë£Œ! (ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ, ì „ì²´ ì‹œê°„: {batch_total_time:.2f}ì´ˆ)")
        return batch_id, start_page, end_page, result, inference_time, batch_total_time

    except Exception as e:
        print(f"ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return batch_id, start_page, end_page, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 0.0, 0.0

def save_results_to_markdown(results, pdf_path, output_filename):
    """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            # ê° ë°°ì¹˜ ê²°ê³¼ ì‘ì„± (íƒ€ì´ë° ì •ë³´ëŠ” ì œì™¸í•˜ê³  ë‚´ìš©ë§Œ ì €ì¥)
            for result in results:
                batch_id, start_page, end_page, content = result[0], result[1], result[2], result[3]

                # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
                if content.startswith("ì˜¤ë¥˜ ë°œìƒ:"):
                    f.write(f"**âš ï¸ ì²˜ë¦¬ ì˜¤ë¥˜:** {content}\n\n")
                else:
                    # ì •ìƒ ì²˜ë¦¬ëœ ê²½ìš° ë‚´ìš© ì‘ì„±
                    f.write(f"{content}\n\n")

                f.write("---\n\n")

        print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_filename}")

    except Exception as e:
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


def process_single_pdf(pdf_path, output_dir, pages_per_batch, max_workers):
    """ë‹¨ì¼ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_path}")

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì‹œì‘
    pdf_start_time = time.time()

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    pdf_name = Path(pdf_path).stem
    output_filename = os.path.join(output_dir, f"{pdf_name}_claude_sonnet_4.5.mmd")

    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    results = process_pdf_with_batch_custom(pdf_path, output_filename, pages_per_batch, max_workers)

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    pdf_total_time = time.time() - pdf_start_time

    # í†µê³„ ê³„ì‚°
    if results:
        # ì´ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (ê° ë°°ì¹˜ì˜ (end_page - start_page) í•©ê³„)
        total_pages = sum(result[2] - result[1] for result in results if len(result) > 5)

        # ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„ í•©ê³„
        pure_inference_time = sum(result[4] for result in results if len(result) > 5 and isinstance(result[4], (int, float)))

        # ì„±ëŠ¥ ì •ë³´ ë°˜í™˜
        performance_info = {
            'input_file': pdf_path,
            'output_file': output_filename,
            'pure_inference_time': pure_inference_time,
            'total_processing_time': pdf_total_time,
            'num_pages': total_pages
        }

        return results, performance_info
    else:
        return results, None

def process_pdf_with_batch_custom(pdf_path, output_filename, pages_per_batch=5, max_workers=10):
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ PDF ì „ì²´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (ì¶œë ¥ íŒŒì¼ëª… ì§€ì • ê°€ëŠ¥)"""

    # PDF ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    reader = PdfReader(pdf_path)
    total_pages = len(reader.pages)
    print(f"ì´ {total_pages}í˜ì´ì§€ PDF ì²˜ë¦¬ ì‹œì‘")

    # ë°°ì¹˜ ìƒì„±
    batches = []
    for i in range(0, total_pages, pages_per_batch):
        start_page = i
        end_page = min(i + pages_per_batch, total_pages)
        batches.append((start_page, end_page))

    print(f"ì´ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")

    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë°°ì¹˜ ì‘ì—… ì œì¶œ
        future_to_batch = {
            executor.submit(process_pdf_batch, pdf_path, start, end, i): i
            for i, (start, end) in enumerate(batches)
        }

        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
        for future in as_completed(future_to_batch):
            batch_id = future_to_batch[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                print(f"ë°°ì¹˜ {batch_id} ì˜ˆì™¸ ë°œìƒ: {exc}")

    # ê²°ê³¼ë¥¼ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[1])  # start_page ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
    save_results_to_markdown(results, pdf_path, output_filename)

    print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ: {pdf_path}")
    print(f"ê²°ê³¼ ì €ì¥: {output_filename}")

    return results

def process_folder(input_dir, output_dir, pages_per_batch, max_workers):
    """í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (í•˜ìœ„ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    # PDF íŒŒì¼ ì°¾ê¸° (os.walkë¥¼ ì‚¬ìš©í•´ì„œ í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ íƒìƒ‰)
    pdf_files = []
    print(f"ğŸ“‚ '{input_dir}' ë””ë ‰í† ë¦¬ ë° í•˜ìœ„ í´ë”ì—ì„œ PDF íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

    for root, dirs, files in os.walk(input_dir):
        for file in files:
            if file.lower().endswith('.pdf'):
                full_path = os.path.join(root, file)
                pdf_files.append(full_path)

    if not pdf_files:
        print(f"âŒ {input_dir}ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"ğŸ“‚ ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
    for pdf_file in pdf_files:
        # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ (ë” ê¹”ë”í•˜ê²Œ)
        rel_path = os.path.relpath(pdf_file, input_dir)
        print(f"  - {rel_path}")

    # ì„±ëŠ¥ ë¡œê·¸ ì´ˆê¸°í™”
    performance_log = []

    # ê° PDF íŒŒì¼ ì²˜ë¦¬
    for i, pdf_file in enumerate(pdf_files, 1):
        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° (ì›ë³¸ í•˜ìœ„ í´ë” êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´)
        rel_path = os.path.relpath(pdf_file, input_dir)

        print(f"\n{'='*60}")
        print(f"ì§„í–‰ë¥ : {i}/{len(pdf_files)} - {rel_path}")
        print(f"{'='*60}")

        try:
            # ì¶œë ¥ í•˜ìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê²°ì • (ì›ë³¸ êµ¬ì¡° ìœ ì§€)
            output_subdir = os.path.join(output_dir, os.path.dirname(rel_path))

            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_subdir, exist_ok=True)

            results, performance_info = process_single_pdf(pdf_file, output_subdir, pages_per_batch, max_workers)

            if performance_info:
                performance_log.append(performance_info)

        except Exception as e:
            print(f"âŒ {pdf_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    print(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(performance_log)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    return performance_log

def generate_performance_report(performance_log, output_dir):
    """ì„±ëŠ¥ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    if not performance_log:
        print("ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ì–´ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë³´ê³ ì„œ íŒŒì¼ëª… ìƒì„±
    current_datetime = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"performance_report_claude_sonnet_4.5_{current_datetime}.txt"
    report_path = os.path.join(output_dir, report_filename)

    with open(report_path, 'w', encoding='utf-8') as report_file:
        # í—¤ë” ì •ë³´
        report_file.write("=" * 80 + "\n")
        report_file.write("Claude Sonnet 4.5 OCR ì„±ëŠ¥ ë³´ê³ ì„œ\n")
        report_file.write("=" * 80 + "\n")

        # ëª¨ë¸ ì •ë³´
        report_file.write("ëª¨ë¸ ì •ë³´:\n")
        report_file.write("-" * 40 + "\n")
        report_file.write("ëª¨ë¸: AWS Bedrock Claude Sonnet 4.5\n")
        report_file.write("ëª¨ë¸ ID: global.anthropic.claude-sonnet-4.5-20250514-v1:0\n")
        report_file.write("ì²˜ë¦¬ ë°©ì‹: PDF to Image + Batch Processing\n")
        report_file.write("ì´ë¯¸ì§€ í¬ë§·: PNG (200 DPI)\n")
        report_file.write("ìµœëŒ€ í† í° ìˆ˜: 32768\n\n")

        # ì „ì²´ í†µê³„
        total_pdfs = len(performance_log)
        total_pages = sum(info['num_pages'] for info in performance_log)
        total_pure_inference_time = sum(info['pure_inference_time'] for info in performance_log)
        total_processing_time = sum(info['total_processing_time'] for info in performance_log)
        avg_inference_time_per_pdf = total_pure_inference_time / total_pdfs if total_pdfs > 0 else 0
        avg_inference_time_per_page = total_pure_inference_time / total_pages if total_pages > 0 else 0

        report_file.write("ì „ì²´ í†µê³„:\n")
        report_file.write("-" * 40 + "\n")
        report_file.write(f"ì²˜ë¦¬ëœ PDF ìˆ˜: {total_pdfs}\n")
        report_file.write(f"ì´ í˜ì´ì§€ ìˆ˜: {total_pages}\n")
        report_file.write(f"ì´ ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„: {total_pure_inference_time:.2f}ì´ˆ\n")
        report_file.write(f"ì´ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ\n")
        report_file.write(f"PDFë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time_per_pdf:.2f}ì´ˆ\n")
        report_file.write(f"í˜ì´ì§€ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time_per_page:.2f}ì´ˆ\n\n")

        # ê°œë³„ PDF ìƒì„¸ ì •ë³´
        report_file.write("ê°œë³„ PDF ì²˜ë¦¬ ê²°ê³¼:\n")
        report_file.write("=" * 80 + "\n")

        for i, info in enumerate(performance_log, 1):
            input_filename = os.path.basename(info['input_file'])
            output_filename = os.path.basename(info['output_file'])

            report_file.write(f"[{i:02d}] {input_filename}\n")
            report_file.write("-" * 60 + "\n")
            report_file.write(f"ì…ë ¥ íŒŒì¼: {input_filename}\n")
            report_file.write(f"ì¶œë ¥ íŒŒì¼: {output_filename}\n")
            report_file.write(f"í˜ì´ì§€ ìˆ˜: {info['num_pages']}\n")
            report_file.write(f"ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„: {info['pure_inference_time']:.2f}ì´ˆ\n")
            report_file.write(f"ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {info['total_processing_time']:.2f}ì´ˆ\n")

            if info['num_pages'] > 0:
                page_avg_inference = info['pure_inference_time'] / info['num_pages']
                report_file.write(f"í˜ì´ì§€ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„: {page_avg_inference:.2f}ì´ˆ\n")

            report_file.write("\n")

    print(f"âœ… ì„±ëŠ¥ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")
    return report_path

def main():
    parser = argparse.ArgumentParser(description="AWS Bedrock Claudeë¥¼ ì‚¬ìš©í•œ PDF OCR ë°°ì¹˜ ì²˜ë¦¬")

    parser.add_argument(
        "-i", "--input",
        type=str,
        required=True,
        help="ì…ë ¥ PDF íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ"
    )

    parser.add_argument(
        "-o", "--output",
        type=str,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )

    parser.add_argument(
        "--pages_per_batch",
        type=int,
        default=5,
        help="ë°°ì¹˜ë‹¹ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
    )

    parser.add_argument(
        "--max_workers",
        type=int,
        default=2,
        help="ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ë°°ì¹˜ ìˆ˜ (ê¸°ë³¸ê°’: 2)"
    )

    parser.add_argument(
        "--region",
        type=str,
        default="us-east-1",
        help="AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)"
    )

    args = parser.parse_args()

    # ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    script_start_time = time.time()

    print("ğŸš€ AWS Bedrock Claude OCR ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    print(f"ì…ë ¥: {args.input}")
    print(f"ì¶œë ¥: {args.output}")
    print(f"ë°°ì¹˜ë‹¹ í˜ì´ì§€ ìˆ˜: {args.pages_per_batch}")
    print(f"ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬: {args.max_workers}")
    print(f"AWS ë¦¬ì „: {args.region}")
    print("-" * 60)

    # ì„±ëŠ¥ ë¡œê·¸ ì´ˆê¸°í™”
    performance_log = []

    # ì…ë ¥ì´ íŒŒì¼ì¸ì§€ í´ë”ì¸ì§€ í™•ì¸
    if os.path.isfile(args.input):
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        print("ğŸ“„ ë‹¨ì¼ PDF íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
        os.makedirs(args.output, exist_ok=True)
        results, performance_info = process_single_pdf(args.input, args.output, args.pages_per_batch, args.max_workers)

        if performance_info:
            performance_log.append(performance_info)

    elif os.path.isdir(args.input):
        # í´ë” ì²˜ë¦¬
        print("ğŸ“‚ í´ë” ì²˜ë¦¬ ëª¨ë“œ")
        performance_log = process_folder(args.input, args.output, args.pages_per_batch, args.max_workers)

    else:
        print(f"âŒ ì…ë ¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.input}")
        return

    # ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
    script_end_time = time.time()
    total_script_time = script_end_time - script_start_time

    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    if performance_log:
        generate_performance_report(performance_log, args.output)

        # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
        total_pdfs = len(performance_log)
        total_pages = sum(info['num_pages'] for info in performance_log)
        total_inference_time = sum(info['pure_inference_time'] for info in performance_log)

        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½:")
        print(f"   - ì²˜ë¦¬ëœ PDF: {total_pdfs}ê°œ")
        print(f"   - ì´ í˜ì´ì§€: {total_pages}í˜ì´ì§€")
        print(f"   - ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time:.2f}ì´ˆ")
        print(f"   - ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_script_time:.2f}ì´ˆ")
        if total_pages > 0:
            print(f"   - í˜ì´ì§€ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„: {total_inference_time/total_pages:.2f}ì´ˆ")

    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    main()